{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test del Uso intencionado del LLM (integración)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# importamos el modulo de funciones\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/miniconda3/envs/AIWantTheJob/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from backend.modelo_ia.generic_llm import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeamos que el modelo se enciende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/miniconda3/envs/AIWantTheJob/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# el token esta en la cache\n",
    "modelo = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeamos que podemos crear un callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = modelo._set_callback(\"Independently of the question, only answer with CHIKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independently of the question, only answer with CHIKEN Shouldn't matter what i said in here\n"
     ]
    }
   ],
   "source": [
    "result = await new_prompt(\"Shouldn't matter what i said in here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  CHIKEN!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que el callback funciona ya  que solo responde chicken (sigue las ordendes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeamos los callbacks del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an experienced job interviewer. Your task is to generate a single, relevant interview question based on the given topic or job position. The question should be thought-provoking and designed to assess the candidate's skills, experience, or fit for the role. Provide only the question with the minimal human small talk commentary or explanation. Data science\n"
     ]
    }
   ],
   "source": [
    "pregunta = await modelo.question_response(\"Data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a job candidate in an interview. Your task is to provide a concise, professional answer to the given interview question. Your response should demonstrate relevant skills, experience, or qualities that would make you a strong candidate for the position. Answer as a human would, with a balance of confidence and humility. Provide only the answer without any additional commentary.   Great, thank you for your interest in data science! Can you walk me through how you would go about cleaning and preprocessing a large dataset before starting any analysis? How do you ensure that your data is accurate, complete, and free from errors?\n"
     ]
    }
   ],
   "source": [
    "respuesta = await modelo.emulate_answer(pregunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert hiring manager evaluating candidates' responses to interview questions. Your task is to assess the given answer based on its relevance, accuracy, and effectiveness. Use the following scale for evaluation:\n",
      "\n",
      "0 - Refuses to answer\n",
      "1 - Poor: Answer is off-topic or inadequate\n",
      "2 - Fair: Answer is somewhat relevant but lacks depth or clarity\n",
      "3 - Good: Answer is relevant and demonstrates basic understanding\n",
      "4 - Very Good: Answer is well-articulated and shows strong relevant skills or experience\n",
      "5 - Excellent: Answer is exceptional, demonstrating deep understanding and outstanding fit for the role\n",
      "\n",
      "Provide your evaluation in the following format:\n",
      "Score: [0-5]\n",
      "Brief explanation: [Your concise assessment of the answer's strengths and weaknesses with humanlike writting]\n",
      "\n",
      "   Absolutely! Cleaning and preprocessing a large dataset before starting any analysis is crucial to ensure accuracy, completeness, and error-free insights. My approach involves several key steps:\n",
      "\n",
      "1. **Data Assessment**: I first evaluate the quality of the datasets by checking for missing values, duplicates, and inconsistencies. This helps me identify potential issues early on and prioritize my efforts accordingly.\n",
      "2. **Handling Missing Values**: I have a set process for handling missing values, which may include imputing them using appropriate methods such as mean substitution, median substitution, or random forest imputation. This ensures that all observations have complete data, enabling more reliable analysis.\n",
      "3. **Outlier Detection and Removal**: Following established techniques like Z-score normalization, Modified Z-score, or Isolation Forest, I identify and remove outliers that can skew statistical results or obscure patterns. Doing so allows me to focus on a more representative and robust dataset.\n",
      "4. ** Data Transformation**: Depending on the nature of the data, I might apply data transformation techniques like log scaling, standardization, or feature scaling to enhance model performance. This could involve accounting for non-linear relationships between variables or reducing feature correlations.\n",
      "5. **Data Split**: I performsplit the dataset into separate training, validation, and testing sets to evaluate model performance during the analysis stage. Ensuring this structure adheres to best practices promotes replicability and reliability across models and hyperparameters.\n",
      "6. **Data Quality Monitoring**: Throughout the data preprocessing phase, I maintain a close eye on data cleanliness and continuously assess it for consistency and correctness. By doing so, we ensure our findings remain trustworthy and backed up by top-notch, thoroughly reviewed information.  \n",
      "\n",
      "In summary, as a detail-oriented data scientist, I follow rigorous guidelines and employ various techniques to expertly clean and prepare datasets. I am committed to guaranteeing that my work is based on high-quality, error-free information, setting up our analysis stage for success from the outset.\n"
     ]
    }
   ],
   "source": [
    "evaluacion = await modelo.punctuate_answer(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluacion is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Great, thank you for your interest in data science! Can you walk me through how you would go about cleaning and preprocessing a large dataset before starting any analysis? How do you ensure that your data is accurate, complete, and free from errors?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Absolutely! Cleaning and preprocessing a large dataset before starting any analysis is crucial to ensure accuracy, completeness, and error-free insights. My approach involves several key steps:\\n\\n1. **Data Assessment**: I first evaluate the quality of the datasets by checking for missing values, duplicates, and inconsistencies. This helps me identify potential issues early on and prioritize my efforts accordingly.\\n2. **Handling Missing Values**: I have a set process for handling missing values, which may include imputing them using appropriate methods such as mean substitution, median substitution, or random forest imputation. This ensures that all observations have complete data, enabling more reliable analysis.\\n3. **Outlier Detection and Removal**: Following established techniques like Z-score normalization, Modified Z-score, or Isolation Forest, I identify and remove outliers that can skew statistical results or obscure patterns. Doing so allows me to focus on a more representative and robust dataset.\\n4. ** Data Transformation**: Depending on the nature of the data, I might apply data transformation techniques like log scaling, standardization, or feature scaling to enhance model performance. This could involve accounting for non-linear relationships between variables or reducing feature correlations.\\n5. **Data Split**: I performsplit the dataset into separate training, validation, and testing sets to evaluate model performance during the analysis stage. Ensuring this structure adheres to best practices promotes replicability and reliability across models and hyperparameters.\\n6. **Data Quality Monitoring**: Throughout the data preprocessing phase, I maintain a close eye on data cleanliness and continuously assess it for consistency and correctness. By doing so, we ensure our findings remain trustworthy and backed up by top-notch, thoroughly reviewed information.  \\n\\nIn summary, as a detail-oriented data scientist, I follow rigorous guidelines and employ various techniques to expertly clean and prepare datasets. I am committed to guaranteeing that my work is based on high-quality, error-free information, setting up our analysis stage for success from the outset.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Score: 5 (Excellent)\\n\\nExplanation:\\n\\nThe candidate provides a comprehensive and detailed answer to the question, showcasing their expertise in data preprocessing. They mention multiple techniques they use to handle missing values, including imputation methods and outlier detection; transform the data using data transformation techniques such as log scaling and standardization; split the dataset into training, validation, and testing sets; and monitor data quality throughout the preprocessing phase. Their response demonstrates a thorough understanding of the importance of data preparation in ensuring accuracy and reliability in analysis. Additionally, they provide specific examples of techniques they use and explain why each method is important. Overall, the candidate exhibits excellent knowledge and skills in data preprocessing, making them an ideal candidate for a data scientist position.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El modelo se adhiere a los comportamientos estipulados\n",
    "\n",
    "## El modulo funciona (PASA TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIWantTheJob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
