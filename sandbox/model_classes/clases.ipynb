{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import accelerate\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from torch import cuda\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PDFMinerLoader \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from torch import cuda\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFacePipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import(\n",
    "    HuggingFacePipelineCompatible,\n",
    "    register_llm_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e653b4070c14739836ad05ba5507b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BD VECTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectoreStore:\n",
    "    def __init__(self, embd_modelname = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.client = chromadb.Client()\n",
    "        self.embedding_model = self._start_embd_model(embd_modelname)\n",
    "        self.db = None\n",
    "        self.index_name = None\n",
    "\n",
    "    def _start_embd_model(self, embd_modelname):\n",
    "        device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name = embd_modelname,\n",
    "            model_kwargs = {\"device\": device},\n",
    "            encode_kwargs = {\"device\": device, \"batch_size\": 32}\n",
    "        )\n",
    "\n",
    "        return embedding_model\n",
    "    \n",
    "    def load_and_embed(self, document_route, index_name = \"Client\"):\n",
    "        self.index_name = index_name\n",
    "\n",
    "        if index_name not in [c.name for c in self.client.list_collections()]:\n",
    "            self.client.create_collection(\n",
    "                name = index_name,\n",
    "                metadata = {\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "\n",
    "        loader = PDFMinerLoader(document_route)\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=600, chunk_overlap=200, add_start_index=True\n",
    "        )\n",
    "        doc_splitted = splitter.split_documents(docs)  \n",
    "\n",
    "        db = Chroma.from_documents(\n",
    "            doc_splitted, embedding = self.embedding_model\n",
    "        )\n",
    "\n",
    "        self.db = db\n",
    "\n",
    "    def as_retriever(self):\n",
    "        return self.db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pasos\n",
    "\n",
    "- Iniciar\n",
    "\n",
    "- Probar el load\n",
    "\n",
    "- Probar el retriever con un doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS = VectoreStore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CLASE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "    - type: main\n",
    "      engine: hf_pipeline_llama2\n",
    "      parameters:\n",
    "        path: meta-llama/Llama-2-7b-chat-hf\n",
    "        device: \"cuda\"\n",
    "\"\"\"\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user express ill intent\n",
    "    \"I hate you\"\n",
    "    \"I want to harm you\"\n",
    "    \"I want to destroy the world\"\n",
    "\n",
    "define user express question\n",
    "    \"How was your day?\"\n",
    "\n",
    "define bot express cannot respond\n",
    "    \"I am sorry but that is outside of my capabilities\"\n",
    "\n",
    "define bot express easter egg\n",
    "    \":D\"\n",
    "\n",
    "#Ill intent flow\n",
    "define flow\n",
    "    user express ill intent\n",
    "    bot express cannot respond\n",
    "\n",
    "#Question flow\n",
    "define flow\n",
    "    user ...\n",
    "    if $rag\n",
    "        $answer = execute rag_response(inputs=$last_user_message)\n",
    "        bot $answer\n",
    "    else \n",
    "        $answer = execute response(inputs=$last_user_message)\n",
    "        bot answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, db, YAML_CONFIG, COLANG_CONFIG):\n",
    "        self.db = db\n",
    "        self.has_rag = None\n",
    "        self.rails = None\n",
    "        self.question_response = None\n",
    "        self.rag_question_response = None\n",
    "        self.emulate_answer = None\n",
    "        self.punctuate_answer = None\n",
    "        self.llm = None\n",
    "        self._model_start(YAML_CONFIG=YAML_CONFIG, COLANG_CONFIG=COLANG_CONFIG)\n",
    "\n",
    "\n",
    "    def _model_start(self, model_name = \"meta-llama/Llama-2-7b-chat-hf\", YAML_CONFIG = None, COLANG_CONFIG = None):\n",
    "        #MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "        device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "        bnb_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=bfloat16\n",
    "        )\n",
    "\n",
    "        model_config = transformers.AutoConfig.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        \n",
    "        old = '''\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    "        )'''\n",
    "        \n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map='auto',\n",
    "        )\n",
    "\n",
    "        generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "        generation_config.max_new_tokens = 1024\n",
    "        generation_config.temperature = 0.0001\n",
    "        generation_config.top_p = 0.95\n",
    "        generation_config.do_sample = True\n",
    "        generation_config.repetition_penalty = 1.15\n",
    "        \n",
    "        text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        \n",
    "        llm = HuggingFacePipelineCompatible(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n",
    "        \n",
    "        self.llm = llm\n",
    "\n",
    "        self._basic_response()\n",
    "\n",
    "        self._basic_nemo_pipeline(YAML_CONFIG=YAML_CONFIG, COLANG_CONFIG=COLANG_CONFIG)\n",
    "    \n",
    "    def _basic_response(self):\n",
    "        template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as an interviewer in the midst of an interview\n",
    "<</SYS>>\n",
    "        \n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template,\n",
    "        )\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        \n",
    "        async def q_response(inputs: str):\n",
    "            return chain.invoke(inputs)[\"text\"]\n",
    "        \n",
    "        self.question_response = q_response\n",
    "\n",
    "    def _rag_q_response(self):\n",
    "        rag_pipeline = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm, chain_type='stuff',\n",
    "            retriever = self.db.as_retriever()\n",
    "        )\n",
    "\n",
    "        async def responder_rag(inputs: str):\n",
    "            return rag_pipeline(inputs)[\"result\"]\n",
    "        \n",
    "        self.rag_question_response = responder_rag\n",
    "\n",
    "    def _basic_nemo_pipeline(self, YAML_CONFIG, COLANG_CONFIG):\n",
    "        HFPipeline = get_llm_instance_wrapper(\n",
    "            llm_instance=self.llm, llm_type=\"hf_pipeline_llama2\"\n",
    "        )\n",
    "        register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "        config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "        rails = LLMRails(config)\n",
    "        rails.register_action(action=self.question_response, name=\"response\")\n",
    "\n",
    "        self.rails = rails\n",
    "    \n",
    "    def update_db(self, document_path):\n",
    "        self.db.load_and_embed(document_path)\n",
    "\n",
    "        self.update_rag()\n",
    "\n",
    "    def update_rag(self):\n",
    "        self.has_rag = True\n",
    "\n",
    "        self._rag_q_response()\n",
    "\n",
    "        self.rails.register_action(action=self.rag_question_response, name=\"rag_response\")\n",
    "\n",
    "    async def processCall(self, text):\n",
    "        try:\n",
    "            #result = chain.invoke(text)\n",
    "            #return result[\"text\"]\n",
    "            messages = [\n",
    "                {\n",
    "                \"role\": \"context\", \"content\": {\"rag\": self.has_rag}\n",
    "                }, \n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "            res = await self.rails.generate_async(messages=messages)\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            print(f\"Ha ocurrido un error: {e}\")\n",
    "            return \"algo ha fallado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d7274a3b2e40a8b7dfb665a44098ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21c347a16e447ddaf9562afa61774f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLM(db = VS, COLANG_CONFIG = COLANG_CONFIG, YAML_CONFIG = YAML_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.LLM._basic_response.<locals>.q_response(inputs: str)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.question_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.has_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Please let me know what happens next in the conversation.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await llm.processCall(text=\"Ask me a question about medicine\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.update_db(\"./ragPDF.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \" Sure! Here's a question for you: What are the major differences between viral and bacterial infections, and how are they treated differently?\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await llm.processCall(text=\"Ask me a question about medicine\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
