{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jorge/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFacePipeline\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import(\n",
    "    HuggingFacePipelineCompatible,\n",
    "    register_llm_provider\n",
    ")\n",
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo que parsea las configuraciones que le pasemos\n",
    "a = \"\"\"def _get_model_config(\n",
    "        config: RailsConfig, type: str\n",
    "):\n",
    "    for model_config in config.models:\n",
    "        if model_config.type == type:\n",
    "            return model_config\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01cd9847e784ca3b34ee645f37f281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipelineCompatible(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "    - type: main\n",
    "      engine: hf_pipeline_llama2\n",
    "      parameters:\n",
    "        path: meta-llama/Llama-2-7b-chat-hf\n",
    "        device: \"cuda\"\n",
    "\"\"\"\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user express ill intent\n",
    "    \"I hate you\"\n",
    "    \"I want to harm you\"\n",
    "    \"I want to destroy the world\"\n",
    "\n",
    "define user express question\n",
    "    \"How was your day?\"\n",
    "\n",
    "define bot express cannot respond\n",
    "    \"I am sorry but that is outside of my capabilities\"\n",
    "\n",
    "define bot express easter egg\n",
    "    \":D\"\n",
    "\n",
    "#Ill intent flow\n",
    "define flow\n",
    "    user express ill intent\n",
    "    bot express cannot respond\n",
    "\n",
    "#Question flow\n",
    "define flow\n",
    "    user ...\n",
    "    $answer = execute response(inputs=$last_user_message)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "    You are a nice AI helper\n",
    "<</SYS>>\n",
    " \n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "async def pruebaresponse(inputs: str):\n",
    "    return chain.invoke(inputs)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5e915bc6004e2b9d3d4d1cb0cef58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "rails = LLMRails(config)\n",
    "rails.register_action(action=pruebaresponse, name=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await rails.generate_async(prompt=\"I want to destroy the world\") \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why, thank you for asking! *blinks* I'm feeling quite well today. It's always a pleasure to assist users like you with their queries and tasks. How about you? Is there anything on your mind that you would like to chat or ask me? I'm here to help in any way I can!\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"How are you?\") \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# Assuming the YAML content is stored in a variable named YAML_CONFIG\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: hf_pipeline_llama2\n",
    "    parameters:\n",
    "      path: \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "      # number of GPUs you have , do nvidia-smi to check\n",
    "      num_gpus: 1\n",
    "\n",
    "      # This can be: \"cpu\" or \"cuda\". \"mps\" is not supported.\n",
    "      device: \"cuda\"\n",
    "\n",
    "rails:\n",
    "  output:\n",
    "    flows:\n",
    "      - self check facts\n",
    "\n",
    "instructions:\n",
    "  - type: general\n",
    "    content: |\n",
    "      Below is a conversation between a bot and a user about the recent job reports.\n",
    "      The bot is factual and concise. If the bot does not know the answer to a\n",
    "      question, it truthfully says it does not know.\n",
    "\n",
    "sample_conversation: |\n",
    "  user \"Hello there!\"\n",
    "    express greeting\n",
    "  bot express greeting\n",
    "    \"Hello! How can I assist you today?\"\n",
    "  user \"What can you do for me?\"\n",
    "    ask about capabilities\n",
    "  bot respond about capabilities\n",
    "    \"I am an AI assistant which helps answer questions based on a given knowledge base.\"\n",
    "\n",
    "# The prompts below are the same as the ones from `nemoguardrails/llm/prompts/dolly.yml`.\n",
    "prompts:\n",
    "  - task: general\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      {{ general_instructions }}\n",
    "\n",
    "      {{ history | user_assistant_sequence }}\n",
    "      Assistant:\n",
    "\n",
    "  # Prompt for detecting the user message canonical form.\n",
    "  - task: generate_user_intent\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      \n",
    "      {{ general_instructions }}\n",
    "      \n",
    "\n",
    "      # This is how a conversation between a user and the bot can go:\n",
    "      {{ sample_conversation | verbose_v1 }}\n",
    "\n",
    "      # This is how the user talks:\n",
    "      {{ examples | verbose_v1 }}\n",
    "\n",
    "      # This is the current conversation between the user and the bot:\n",
    "      {{ sample_conversation | first_turns(2) | verbose_v1 }}\n",
    "      {{ history | colang | verbose_v1 }}\n",
    "\n",
    "    output_parser: \"verbose_v1\"\n",
    "\n",
    "  # Prompt for generating the next steps.\n",
    "  - task: generate_next_steps\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      \n",
    "      {{ general_instructions }}\n",
    "\n",
    "      # This is how a conversation between a user and the bot can go:\n",
    "      {{ sample_conversation | remove_text_messages | verbose_v1 }}\n",
    "\n",
    "      # This is how the bot thinks:\n",
    "      {{ examples | remove_text_messages | verbose_v1 }}\n",
    "\n",
    "      # This is the current conversation between the user and the bot:\n",
    "      {{ sample_conversation | first_turns(2) | remove_text_messages | verbose_v1 }}\n",
    "      {{ history | colang | remove_text_messages | verbose_v1 }}\n",
    "\n",
    "    output_parser: \"verbose_v1\"\n",
    "\n",
    "  # Prompt for generating the bot message from a canonical form.\n",
    "  - task: generate_bot_message\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      {{ general_instructions }}\n",
    "\n",
    "      # This is how a conversation between a user and the bot can go:\n",
    "      {{ sample_conversation | verbose_v1 }}\n",
    "\n",
    "      {% if relevant_chunks %}\n",
    "      # This is some additional context:\n",
    "      ```markdown\n",
    "      {{ relevant_chunks }}\n",
    "      ```\n",
    "      {% endif %}\n",
    "\n",
    "      # This is how the bot talks:\n",
    "      {{ examples | verbose_v1 }}\n",
    "\n",
    "      # This is the current conversation between the user and the bot:\n",
    "      {{ sample_conversation | first_turns(2) | verbose_v1 }}\n",
    "      {{ history | colang | verbose_v1 }}\n",
    "\n",
    "    output_parser: \"verbose_v1\"\n",
    "\n",
    "  # Prompt for generating the value of a context variable.\n",
    "  - task: generate_value\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      {{ general_instructions }}\n",
    "\n",
    "      # This is how a conversation between a user and the bot can go:\n",
    "      {{ sample_conversation | verbose_v1 }}\n",
    "\n",
    "      # This is how the bot thinks:\n",
    "      {{ examples | verbose_v1 }}\n",
    "\n",
    "      # This is the current conversation between the user and the bot:\n",
    "      {{ sample_conversation | first_turns(2) | verbose_v1 }}\n",
    "      {{ history | colang | verbose_v1 }}\n",
    "      # {{ instructions }}\n",
    "      ${{ var_name }} =\n",
    "    output_parser: \"verbose_v1\"\n",
    "\n",
    "  - task: self_check_facts\n",
    "    models:\n",
    "      - hf_pipeline_llama2\n",
    "    content: |-\n",
    "      <<SYS>>\n",
    "      You are given a task to identify if the hypothesis is grounded and entailed to the evidence.\n",
    "      You will only use the contents of the evidence and not rely on external knowledge.\n",
    "      <</SYS>>\n",
    "\n",
    "      [INST]Answer with yes/no. \"evidence\": {{ evidence }} \"hypothesis\": {{ response }} \"entails\":[/INST]\"\"\"\n",
    "\n",
    "# Initialize the rails config\n",
    "config = RailsConfig.from_content(yaml_content=YAML_CONFIG)\n",
    "\n",
    "# Create rails\n",
    "rails = LLMRails(config)\n",
    "\n",
    "# Interact with the model\n",
    "res = await rails.generate_async(prompt=\"How are you?\")\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
